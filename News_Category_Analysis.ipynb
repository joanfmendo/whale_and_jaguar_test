{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "News_Category_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGUH241DBIbN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Install libraries\n",
        "#pip install langdetect\n",
        "#pip install spacy\n",
        "!python3 -m spacy download es_core_news_sm\n",
        "!python3 -m spacy download en_core_web_sm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHSaQMaCBWka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load libraries\n",
        "from google.colab import drive #to use Google Drive\n",
        "import time\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk #for NLP\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.corpus import cmudict\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import re #regular expressions\n",
        "import spacy\n",
        "import es_core_news_sm\n",
        "import en_core_web_sm\n",
        "from langdetect import detect\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "import collections as coll\n",
        "import math\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('cmudict')\n",
        "\n",
        "#mount google drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT0sjoJzBu0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####### FUNCTIONS FOR DATA PREPARATION\n",
        "\n",
        "# Standardize separators\n",
        "def standard_sep(text,sep):\n",
        "    try:\n",
        "      punctuation = {'\\r\\n','\\n','\\t',' and ',', ','contributor',str(sep+'and '),' (',' & ',' | ','- '}\n",
        "      for sign in punctuation:\n",
        "        text = text.lower().replace(' ',' ').replace('  ',' ').replace('  ',' ').replace('  ',' ').replace('  ',' ') #corrects space sinonymia\n",
        "        text = text.replace(sign, sep) #replaces signs\n",
        "        text = text.replace(str(sep+sep), sep).replace(str(sep+sep), sep).replace(str(sep+sep), sep).replace(str(sep+sep), sep).replace(str(sep+sep), sep) #removes duplicated separators            \n",
        "      return text\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# Remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    try:\n",
        "        punctuation = {'/', '\"', '(', ')', '.', ',', '%', ';', '?', '¿', '!', '¡',\n",
        "                       ':', '#', '$', '&', '>', '<', '-', '_', '°', '|', '¬', '\\\\', '*', '+',\n",
        "                       '[', ']', '{', '}', '=', \"'\", '@','‘','’','—','«','»','“','”'}\n",
        "        for sign in punctuation:\n",
        "            text = text.replace(sign, ' ') #replaces signs\n",
        "        text = text.replace('  ', ' ').replace('  ', ' ').replace('  ', ' ') #removes duplicated spaces\n",
        "        \n",
        "        return text\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# Cleaning text function\n",
        "def clean_text(text, lang):\n",
        "  try:\n",
        "    outputtext=text.lower() #converts to lowercase\n",
        "    outputtext=re.sub(\" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)\", \"\", outputtext) #removes URLs\n",
        "    outputtext = remove_punctuation(outputtext) #removes punctuation\n",
        "    outputtext = ' '.join([word for word in outputtext.split() if word not in stopwords.words(lang)]) #removes stopwords\n",
        "    return outputtext\n",
        "  except:\n",
        "    return None\n",
        "\n",
        "# Stemming function\n",
        "\n",
        "def stem_text(text, lang):\n",
        "    try:\n",
        "      stemmer = SnowballStemmer(lang)\n",
        "      stemmed_list = []\n",
        "      for word in text.split():\n",
        "        stemmed_word = stemmer.stem(word)\n",
        "        stemmed_list.append(stemmed_word)\n",
        "      outputtext = ' '.join([w for w in stemmed_list]) #removes stopwords\n",
        "      return outputtext\n",
        "    except:\n",
        "      return None\n",
        "\n",
        "# Lemmatization function\n",
        "def lemma_text(text, lang):\n",
        "  try:\n",
        "    if lang == 'spanish': nlp = es_core_news_sm.load()\n",
        "    if lang == 'english': nlp = en_core_web_sm.load()\n",
        "    \n",
        "    doc = nlp(text)\n",
        "    lemmas = [tok.lemma_.lower() for tok in doc]\n",
        "    outputtext = ' '.join([w for w in lemmas]) #removes stopwords\n",
        "    return outputtext\n",
        "  except:\n",
        "      return None\n",
        "\n",
        "# Laguage detection\n",
        "def detect_lang(text, default_lang):\n",
        "  try:\n",
        "    output = detect(doc)\n",
        "  except:\n",
        "    output = default_lang\n",
        "  if output == 'es': output = 'spanish'\n",
        "  if output == 'en': output = 'english'\n",
        "  return output\n",
        "\n",
        "# -----------------------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------------------\n",
        "####### FUNCTIONS FOR TEXT CLUSTERING\n",
        "\n",
        "# Best LDA model\n",
        "def best_LDA(search_params, documents):\n",
        "  # vectorize data\n",
        "  data_vectorized = CountVectorizer(max_df=0.95, min_df=2).fit_transform(documents)\n",
        "  # Init Grid Search Class\n",
        "  model = GridSearchCV(LatentDirichletAllocation(), param_grid=search_params)\n",
        "  # Do the Grid Search\n",
        "  model.fit(data_vectorized)\n",
        "  # Model Parameters\n",
        "  best_model_params = model.best_params_\n",
        "  # Log Likelihood Score\n",
        "  best_model_score =  model.best_score_\n",
        "  # Perplexity\n",
        "  best_model_perplexity =  model.best_estimator_.perplexity(data_vectorized)\n",
        "  return best_model_params, best_model_score, best_model_perplexity\n",
        "\n",
        "# Clean strings and returning an array of ngrams\n",
        "def ngrams_analyzer(string, N = 3):\n",
        "    string = re.sub(r'[,-./]', r'', string)\n",
        "    ngrams = zip(*[string[i:] for i in range(N)])  # N-Gram length is N\n",
        "    return [''.join(ngram) for ngram in ngrams]\n",
        "\n",
        "# Calculates TF IDF matrix and features\n",
        "def tf_idf_transformer(documents):\n",
        "  # Calculate term frequency matrix\n",
        "  vectorizer = CountVectorizer(max_df=0.95, min_df=2)\n",
        "  #vectorizer = TfidfVectorizer(analyzer=ngrams_analyzer)\n",
        "  tf_idf = vectorizer.fit_transform(documents)\n",
        "  # Obtain feature names\n",
        "  feature_names = vectorizer.get_feature_names()\n",
        "  return tf_idf, feature_names\n",
        "\n",
        "# Create LDA model\n",
        "def LDA_model(documents, n_topics, n_words):\n",
        "  # Get TF-IDF matrix and feature names\n",
        "  tf_idf, feature_names = tf_idf_transformer(documents)\n",
        "  # Run LDA\n",
        "  lda_model = LatentDirichletAllocation(n_components = n_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf_idf)\n",
        "  topic_w = lda_model.transform(tf_idf) #weights for each topic\n",
        "  word_w = lda_model.components_ #weights for each word in tf matrix\n",
        "  # Extract most frequent topic for each document\n",
        "  mft = []\n",
        "  for weights in topic_w.tolist():\n",
        "    mft.append(weights.index(max(weights)))\n",
        "  # Return most frequent topics\n",
        "  topics = []\n",
        "  topic_words = []\n",
        "  for topic_idx, topic in enumerate(word_w):\n",
        "    words = \" \".join([feature_names[i] for i in topic.argsort()[:-n_words - 1:-1]]) # calculates most frequent words per each topic\n",
        "    topics.append(topic_idx)\n",
        "    topic_words.append(words)\n",
        "  mfw = pd.DataFrame(list(zip(topics, topic_words)), columns =['most_freq_topic', 'most_freq_words'])\n",
        "  return mft, mfw\n",
        "\n",
        "# create kMeans model\n",
        "def kMeans_ElbowMethod(data_as_list, kmin, kmax):\n",
        "    X = data_as_list  # <your_data>\n",
        "    distorsions = []\n",
        "    for k in range(kmin, kmax):\n",
        "        kmeans = KMeans(n_clusters=k)\n",
        "        kmeans.fit(X)\n",
        "        distorsions.append(kmeans.inertia_)\n",
        "\n",
        "    fig = plt.figure(figsize=(15, 5))\n",
        "    plt.plot(range(kmin, kmax), distorsions, 'bo-')\n",
        "    plt.grid(True)\n",
        "    plt.ylabel(\"Square Root Error\")\n",
        "    plt.xlabel(\"Number of Clusters\")\n",
        "    plt.title('Elbow curve')\n",
        "    #plt.savefig(\"ElbowCurve.png\")\n",
        "    plt.show()\n",
        "\n",
        "def PCA_transformer(numeric_data, c_level):\n",
        "  np_data = (np.array(numeric_data))\n",
        "\t# mean normalization of the data . converting into normal distribution having mean=0 , -0.1<x<0.1\n",
        "  sc = StandardScaler()\n",
        "  x = sc.fit_transform(np_data)\n",
        "  # setting number of components\n",
        "  pca = PCA().fit(x)\n",
        "  # set number of components for PCA\n",
        "  csum = np.cumsum(pca.explained_variance_ratio_)\n",
        "  nc=0\n",
        "  for c in csum:\n",
        "    nc = nc+1\n",
        "    if c > 0.9: break\n",
        "  # find principal components\n",
        "  pca = PCA(n_components=nc)\n",
        "  components = (pca.fit_transform(x))\n",
        "  return components\n",
        "\n",
        "def kMeans_clusters(numeric_data, K):\n",
        "    # reduce dimensionality with PCA\n",
        "    components = PCA_transformer(numeric_data, c_level = .9)\n",
        "\t# Apply kmeans algorithm\n",
        "    kmeans = KMeans(n_clusters=K, n_jobs=-1)\n",
        "    kmeans.fit_transform(components)\n",
        "    labels = kmeans.labels_\n",
        "    return labels\n",
        "\n",
        "def kMeans_topics(documents, n_topics):\n",
        "  # Calculate TF-IDF matrix\n",
        "  vectorizer = TfidfVectorizer(analyzer=ngrams_analyzer)\n",
        "  tf_idf = vectorizer.fit_transform(documents)\n",
        "  # Run k-Means\n",
        "  kmeans = KMeans(n_clusters=n_topics).fit(tf_idf)\n",
        "  # Obtain clusters\n",
        "  clusters = kmeans.predict(vectorizer.transform(documents)).tolist()\n",
        "  return clusters\n",
        "# -----------------------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------------------\n",
        "####### FUNCTIONS FOR STYLOMETRIC FEATURE EXTRACTION\n",
        "\n",
        "# returns a feature vector of text\n",
        "def FeatureExtraction(texts, winSize, step):\n",
        "    # cmu dictionary for syllables\n",
        "    global cmuDictionary\n",
        "    cmuDictionary = cmudict.dict()\n",
        "    vector = []\n",
        "    for doc in texts:\n",
        "        feature = []\n",
        "        tokens = word_tokenize(doc.lower(), language='english') #tokenize words\n",
        "        sent_tokens = sent_tokenize(doc, language='english') #tokenize sentences\n",
        "        words_wosp = RemoveStopwords(tokens, language='english') #remove stopwords\n",
        "        words_woschs = RemoveSpecialCHs(tokens) # remove special characters\n",
        "        len_words_woschs = len(words_woschs)\n",
        "        \n",
        "        # LEXICAL FEATURES\n",
        "        meanwl = (Avg_wordLength(words_wosp))\n",
        "        feature.append(meanwl)\n",
        "\n",
        "        meansl = (Avg_SentLenghtByCh(sent_tokens))\n",
        "        feature.append(meansl)\n",
        "\n",
        "        mean = (Avg_SentLenghtByWord(sent_tokens))\n",
        "        feature.append(mean)\n",
        "\n",
        "        meanSyllable = Avg_Syllable_per_Word(words_wosp)\n",
        "        feature.append(meanSyllable)\n",
        "\n",
        "        means = CountSpecialCharacter(doc)\n",
        "        feature.append(means)\n",
        "\n",
        "        p = CountPunctuation(doc)\n",
        "        feature.append(p)\n",
        "\n",
        "        f = CountFunctionalWords(words_woschs, len_words_woschs)\n",
        "        feature.append(f)\n",
        "\n",
        "        # VOCABULARY RICHNESS FEATURES\n",
        "\n",
        "        TTratio = typeTokenRatio(tokens)\n",
        "        feature.append(TTratio)\n",
        "\n",
        "        HonoreMeasureR, hapax = hapaxLegemena(words_woschs, len_words_woschs)\n",
        "        feature.append(hapax)\n",
        "        feature.append(HonoreMeasureR)\n",
        "\n",
        "        SichelesMeasureS, dihapax = hapaxDisLegemena(words_woschs, len_words_woschs)\n",
        "        feature.append(dihapax)\n",
        "        feature.append(SichelesMeasureS)\n",
        "\n",
        "        YuleK = YulesCharacteristicK(words_woschs, len_words_woschs)\n",
        "        feature.append(YuleK)\n",
        "\n",
        "        S = SimpsonsIndex(words_woschs, len_words_woschs)\n",
        "        feature.append(S)\n",
        "\n",
        "        B = BrunetsMeasureW(words_woschs, len_words_woschs)\n",
        "        feature.append(B)\n",
        "\n",
        "        Shannon = ShannonEntropy(words_woschs, len_words_woschs)\n",
        "        feature.append(Shannon)\n",
        "\n",
        "        # READIBILTY FEATURES\n",
        "        FR = FleschReadingEase(words_woschs, len_words_woschs, winSize)\n",
        "        feature.append(FR)\n",
        "\n",
        "        FC = FleschCincadeGradeLevel(words_woschs, len_words_woschs, winSize)\n",
        "        feature.append(FC)\n",
        "\n",
        "        D = dale_chall_readability_formula(words_woschs, len_words_woschs, winSize)\n",
        "        feature.append(D)\n",
        "\n",
        "        G = GunningFoxIndex(words_woschs, len_words_woschs, winSize)\n",
        "        feature.append(G)\n",
        "\n",
        "        vector.append(feature)\n",
        "\n",
        "    return vector\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "def RemoveStopwords(tokens, language='english'):\n",
        "    st = [\",\", \".\", \"'\", \"!\", '\"', \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \"-\", \".\", \"/\", \":\", \";\", \"<\", \"=\", '>', \"?\",\n",
        "          \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", '`', \"{\", \"|\", \"}\", '~', '\\t', '\\n']\n",
        "    stop = stopwords.words(language) + st\n",
        "    words_wosp = [word for word in tokens if word not in stop]\n",
        "    return words_wosp\n",
        "# ------------------------------------------------------------------------\n",
        "def RemoveSpecialCHs(tokens):\n",
        "    st = [\",\", \".\", \"'\", \"!\", '\"', \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \"-\", \".\", \"/\", \":\", \";\", \"<\", \"=\", '>', \"?\",\n",
        "          \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", '`', \"{\", \"|\", \"}\", '~', '\\t', '\\n']\n",
        "    words_woschs = [word for word in tokens if word not in st]\n",
        "    return words_woschs\n",
        "# ---------------------------------------------------------------------\n",
        "def syllable_count_Manual(word):\n",
        "  try:\n",
        "    word = word.lower()\n",
        "    count = 0\n",
        "    vowels = \"aeiouy\"\n",
        "    if word[0] in vowels:\n",
        "        count += 1\n",
        "    for index in range(1, len(word)):\n",
        "        if word[index] in vowels and word[index - 1] not in vowels:\n",
        "            count += 1\n",
        "            if word.endswith(\"e\"):\n",
        "                count -= 1\n",
        "    if count == 0:\n",
        "        count += 1\n",
        "    return count\n",
        "  except:\n",
        "    return -1\n",
        "# ---------------------------------------------------------------------\n",
        "# COUNTS NUMBER OF SYLLABLES\n",
        "def syllable_count(word):\n",
        "    global cmuDictionary\n",
        "    d = cmuDictionary\n",
        "    try:\n",
        "        syl = [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0]\n",
        "    except:\n",
        "        syl = syllable_count_Manual(word)\n",
        "    return syl\n",
        "    # ----------------------------------------------------------------------------\n",
        "# removing stop words plus punctuation.\n",
        "def Avg_wordLength(words_wosp):\n",
        "  try:\n",
        "    awl = np.average([len(word) for word in words_wosp])\n",
        "    return awl\n",
        "  except:\n",
        "    return -1\n",
        "# ----------------------------------------------------------------------------\n",
        "# returns avg number of characters in a sentence\n",
        "def Avg_SentLenghtByCh(sent_tokens):\n",
        "  try:\n",
        "    aslbch = np.average([len(token) for token in sent_tokens])\n",
        "    return aslbch\n",
        "  except:\n",
        "    return -1\n",
        "# ----------------------------------------------------------------------------\n",
        "# returns avg number of words in a sentence\n",
        "def Avg_SentLenghtByWord(sent_tokens):\n",
        "  try:\n",
        "    aslbw = np.average([len(token.split()) for token in sent_tokens])\n",
        "    return aslbw\n",
        "  except:\n",
        "    return -1\n",
        "# ----------------------------------------------------------------------------\n",
        "# GIVES NUMBER OF SYLLABLES PER WORD\n",
        "def Avg_Syllable_per_Word(words_wosp):\n",
        "  try:\n",
        "    syllabls = [syllable_count(word) for word in words_wosp]\n",
        "    p = (\" \".join(words_wosp))\n",
        "    aspw = sum(syllabls) / max(1, len(words_wosp))\n",
        "    return aspw\n",
        "  except:\n",
        "    return -1\n",
        "# -----------------------------------------------------------------------------\n",
        "# COUNTS SPECIAL CHARACTERS NORMALIZED OVER LENGTH OF doc\n",
        "def CountSpecialCharacter(text):\n",
        "  try:\n",
        "    st = [\"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \"-\", \"/\", \"<\", \"=\", '>',\n",
        "          \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", '`', \"{\", \"|\", \"}\", '~', '\\t', '\\n']\n",
        "    count = 0\n",
        "    for i in text:\n",
        "        if (i in st):\n",
        "            count = count + 1\n",
        "    sch = count / len(text)\n",
        "    return sch\n",
        "  except:\n",
        "    return -1\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "def CountPunctuation(text):\n",
        "  try:\n",
        "    st = [\",\", \".\", \"'\", \"!\", '\"', \";\", \"?\", \":\", \";\"]\n",
        "    count = 0\n",
        "    for i in text:\n",
        "        if (i in st):\n",
        "            count = count + 1\n",
        "    cp = float(count) / float(len(text))\n",
        "    return cp\n",
        "  except:\n",
        "    return -1\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# RETURNS NORMALIZED COUNT OF FUNCTIONAL WORDS FROM A Framework for\n",
        "# Authorship Identification of Online Messages: Writing-Style Features and Classification Techniques\n",
        "def CountFunctionalWords(words_woschs, len_words_woschs):\n",
        "  try:\n",
        "    functional_words = \"\"\"a between in nor some upon\n",
        "    about both including nothing somebody us\n",
        "    above but inside of someone used\n",
        "    after by into off something via\n",
        "    all can is on such we\n",
        "    although cos it once than what\n",
        "    am do its one that whatever\n",
        "    among down latter onto the when\n",
        "    an each less opposite their where\n",
        "    and either like or them whether\n",
        "    another enough little our these which\n",
        "    any every lots outside they while\n",
        "    anybody everybody many over this who\n",
        "    anyone everyone me own those whoever\n",
        "    anything everything more past though whom\n",
        "    are few most per through whose\n",
        "    around following much plenty till will\n",
        "    as for must plus to with\n",
        "    at from my regarding toward within\n",
        "    be have near same towards without\n",
        "    because he need several under worth\n",
        "    before her neither she unless would\n",
        "    behind him no should unlike yes\n",
        "    below i nobody since until you\n",
        "    beside if none so up your\n",
        "    \"\"\"\n",
        "    functional_words = functional_words.split()\n",
        "    count = 0\n",
        "    for i in words_woschs:\n",
        "        if i in functional_words:\n",
        "            count += 1\n",
        "    fw = count / len_words_woschs\n",
        "    return fw\n",
        "  except:\n",
        "    return -1\n",
        "# ---------------------------------------------------------------------------\n",
        "# also returns Honore Measure R\n",
        "def hapaxLegemena(words_woschs, N):\n",
        "  try:\n",
        "    V1 = 0\n",
        "    # dictionary comprehension . har word kay against value 0 kardi\n",
        "    freqs = {key: 0 for key in words_woschs}\n",
        "    for word in words_woschs:\n",
        "        freqs[word] += 1\n",
        "    for word in freqs:\n",
        "        if freqs[word] == 1:\n",
        "            V1 += 1\n",
        "    V = float(len(set(words_woschs)))\n",
        "    R = 100 * math.log(N) / max(1, (1 - (V1 / V)))\n",
        "    h = V1 / N\n",
        "    return R, h\n",
        "  except:\n",
        "    return -1, -1\n",
        "# ---------------------------------------------------------------------------\n",
        "def hapaxDisLegemena(words_woschs, len_words_woschs):\n",
        "  try:\n",
        "    count = 0\n",
        "    # Collections as coll Counter takes an iterable collapse duplicate and counts as\n",
        "    # a dictionary how many equivelant items has been entered\n",
        "    freqs = coll.Counter()\n",
        "    freqs.update(words_woschs)\n",
        "    for word in freqs:\n",
        "        if freqs[word] == 2:\n",
        "            count += 1\n",
        "\n",
        "    h = count / float(len_words_woschs)\n",
        "    S = count / float(len(set(words_woschs)))\n",
        "    return S, h\n",
        "  except:\n",
        "    return -1,-1\n",
        "# ---------------------------------------------------------------------------\n",
        "# c(w)  = ceil (log2 (f(w*)/f(w))) f(w*) frequency of most commonly used words f(w) frequency of word w\n",
        "# measure of vocabulary richness and connected to zipfs law, f(w*) const rak kay zips law say rank nikal rahay hein\n",
        "def AvgWordFrequencyClass(words_woschs):\n",
        "  try:\n",
        "    # dictionary comprehension . har word kay against value 0 kardi\n",
        "    freqs = {key: 0 for key in words_woschs}\n",
        "    for word in words_woschs:\n",
        "        freqs[word] += 1\n",
        "    maximum = float(max(list(freqs.values())))\n",
        "    awfc = np.average([math.floor(math.log((maximum + 1) / (freqs[word]) + 1, 2)) for word in words_woschs])\n",
        "    return awfc\n",
        "  except:\n",
        "    return -1\n",
        "# --------------------------------------------------------------------------\n",
        "# TYPE TOKEN RATIO NO OF DIFFERENT WORDS / NO OF WORDS\n",
        "def typeTokenRatio(tokens):\n",
        "  try:\n",
        "    ttr = len(set(tokens)) / len(tokens)\n",
        "    return ttr\n",
        "  except:\n",
        "    return -1\n",
        "# --------------------------------------------------------------------------\n",
        "# logW = V-a/log(N)\n",
        "# N = total words , V = vocabulary richness (unique words) ,  a=0.17\n",
        "# we can convert into log because we are only comparing different texts\n",
        "def BrunetsMeasureW(words_woschs, N):\n",
        "  try:\n",
        "    a = 0.17\n",
        "    V = float(len(set(words_woschs)))\n",
        "    B = (V - a) / (math.log(N))\n",
        "    return B\n",
        "  except:\n",
        "    return -1\n",
        "# -------------------------------------------------------------------------\n",
        "# K  10,000 * (M - N) / N**2\n",
        "# , where M  Sigma i**2 * Vi.\n",
        "def YulesCharacteristicK(words_woschs, N):\n",
        "  try:\n",
        "    freqs = coll.Counter()\n",
        "    freqs.update(words_woschs)\n",
        "    vi = coll.Counter()\n",
        "    vi.update(freqs.values())\n",
        "    M = sum([(value * value) * vi[value] for key, value in freqs.items()])\n",
        "    K = 10000 * (M - N) / math.pow(N, 2)\n",
        "    return K\n",
        "  except:\n",
        "    return -1\n",
        "# -------------------------------------------------------------------------\n",
        "# -1*sigma(pi*lnpi)\n",
        "# Shannon and sympsons index are basically diversity indices for any community\n",
        "def ShannonEntropy(words_woschs, lenght):\n",
        "  try:\n",
        "    freqs = coll.Counter()\n",
        "    freqs.update(words_woschs)\n",
        "    arr = np.array(list(freqs.values()))\n",
        "    distribution = 1. * arr\n",
        "    distribution /= max(1, lenght)\n",
        "    import scipy as sc\n",
        "    H = sc.stats.entropy(distribution, base=2)\n",
        "    # H = sum([(i/lenght)*math.log(i/lenght,math.e) for i in freqs.values()])\n",
        "    return H\n",
        "  except:\n",
        "    return -1\n",
        "# ------------------------------------------------------------------\n",
        "# 1 - (sigma(n(n - 1))/N(N-1)\n",
        "# N is total number of words\n",
        "# n is the number of each type of word\n",
        "def SimpsonsIndex(words_woschs, N):\n",
        "  try:\n",
        "    freqs = coll.Counter()\n",
        "    freqs.update(words_woschs)\n",
        "    n = sum([1.0 * i * (i - 1) for i in freqs.values()])\n",
        "    D = 1 - (n / (N * (N - 1)))\n",
        "    return D\n",
        "  except:\n",
        "    return -1\n",
        "# ------------------------------------------------------------------\n",
        "def FleschReadingEase(words_woschs, l, NoOfsentences):\n",
        "  try:\n",
        "    scount = 0\n",
        "    for word in words_woschs:\n",
        "        scount += syllable_count(word)\n",
        "    I = 206.835 - 1.015 * (l / float(NoOfsentences)) - 84.6 * (scount / float(l))\n",
        "    return I\n",
        "  except:\n",
        "    return -1\n",
        "# -------------------------------------------------------------------\n",
        "def FleschCincadeGradeLevel(words_woschs, l, NoOfSentences):\n",
        "  try:\n",
        "    scount = 0\n",
        "    for word in words_woschs:\n",
        "        scount += syllable_count(word)\n",
        "    F = 0.39 * (l / NoOfSentences) + 11.8 * (scount / float(l)) - 15.59\n",
        "    return F\n",
        "  except:\n",
        "    return -1\n",
        "# -----------------------------------------------------------------\n",
        "def dale_chall_readability_formula(words_woschs, NoOfWords, NoOfSectences):\n",
        "  try:\n",
        "    difficult = 0\n",
        "    adjusted = 0\n",
        "    with open('/gdrive/My Drive/My Developments/Whale and Jaguar/dale-chall.pkl', 'rb') as f:\n",
        "        fimiliarWords = pickle.load(f)\n",
        "    for word in words_woschs:\n",
        "        if word not in fimiliarWords:\n",
        "            difficult += 1\n",
        "    percent = (difficult / NoOfWords) * 100\n",
        "    if (percent > 5):\n",
        "        adjusted = 3.6365\n",
        "    D = 0.1579 * (percent) + 0.0496 * (NoOfWords / NoOfSectences) + adjusted\n",
        "    return D\n",
        "  except:\n",
        "    return -1\n",
        "# ------------------------------------------------------------------\n",
        "def GunningFoxIndex(words_woschs, len_words_woschs, NoOfSentences):\n",
        "  try:\n",
        "    NoOFWords = float(len_words_woschs)\n",
        "    complexWords = 0\n",
        "    for word in words_woschs:\n",
        "        if (syllable_count(word) > 2):\n",
        "            complexWords += 1\n",
        "    G = 0.4 * ((NoOFWords / NoOfSentences) + 100 * (complexWords / NoOFWords))\n",
        "    return G\n",
        "  except:\n",
        "    return -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMsv916jB45S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read docs\n",
        "#df = pd.read_json (r'/gdrive/My Drive/My Developments/Whale and Jaguar/News_Category_Dataset_v2.json', lines=True)\n",
        "df = pd.read_excel('/gdrive/My Drive/My Developments/Whale and Jaguar/News_Category_Dataset_stylometry.xlsx', sheet_name='Sheet1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ws9y-1SiB9vC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## DATA TRANSFORMATION\n",
        "\n",
        "# standardize separator from \"authors\" field\n",
        "sep = ', '\n",
        "auth = []\n",
        "main_author = []\n",
        "\n",
        "for line in df['authors']:\n",
        "  a = standard_sep(line,sep)\n",
        "  a = a.split(sep) \n",
        "  auth.append(a)\n",
        "  main_author.append(a[0])\n",
        "\n",
        "df['authors_sep'] = auth #separate authors\n",
        "df['main_author'] = main_author #identify main author\n",
        "df['text'] = df['headline'] + \". \" + df['short_description'] #join texts\n",
        "'''\n",
        "# Detect language\n",
        "job_lang = []\n",
        "for doc in df['text']:\n",
        "  jl = detect_lang(doc, default_lang='english')\n",
        "  job_lang.append(jl)\n",
        "df['language'] = job_lang\n",
        "'''\n",
        "texts_clean = []\n",
        "texts_stem = []\n",
        "text_lemma = []\n",
        "count = 0\n",
        "for doc in df['text'].values.tolist():\n",
        "  # Remove stopwords and other cleansing\n",
        "  doc_clean = clean_text(doc, lang='english')\n",
        "  texts_clean.append(doc_clean)\n",
        "  #Stemma\n",
        "  doc_stem = stem_text(doc_clean, lang='english')\n",
        "  texts_stem.append(doc_stem)\n",
        "  #Lemma\n",
        "  doc_lemma = lemma_text(doc_clean, lang='english')\n",
        "  texts_lemma.append(doc_stem)\n",
        "  count = count + 1\n",
        "  if count % 1000 == 0:\n",
        "    print(\"line \"+str(count)+ \" advance \"+str(round(100*count/len(df),2))+\"%\")\n",
        "\n",
        "df['text_clean'] = texts_clean\n",
        "df['text_stem'] = texts_stem\n",
        "df['text_lemma'] = texts_lemma\n",
        "\n",
        "# merge some classes\n",
        "df['category_merged']=df['category'].replace({\"HEALTHY LIVING\": \"WELLNESS\",\n",
        "                                              \"QUEER VOICES\": \"GROUPS VOICES\",\n",
        "                                              \"BUSINESS\": \"BUSINESS & FINANCES\",\n",
        "                                              \"PARENTS\": \"PARENTING\",\n",
        "                                              \"BLACK VOICES\": \"GROUPS VOICES\",\n",
        "                                              \"THE WORLDPOST\": \"WORLD NEWS\",\n",
        "                                              \"STYLE\": \"STYLE & BEAUTY\",\n",
        "                                              \"GREEN\": \"ENVIRONMENT\",\n",
        "                                              \"TASTE\": \"FOOD & DRINK\",\n",
        "                                              \"WORLDPOST\": \"WORLD NEWS\",\n",
        "                                              \"SCIENCE\": \"SCIENCE & TECH\",\n",
        "                                              \"TECH\": \"SCIENCE & TECH\",\n",
        "                                              \"MONEY\": \"BUSINESS & FINANCES\",\n",
        "                                              \"ARTS\": \"ARTS & CULTURE\",\n",
        "                                              \"COLLEGE\": \"EDUCATION\",\n",
        "                                              \"LATINO VOICES\": \"GROUPS VOICES\",\n",
        "                                              \"CULTURE & ARTS\": \"ARTS & CULTURE\",\n",
        "                                              \"FIFTY\": \"MISCELLANEOUS\",\n",
        "                                              \"GOOD NEWS\": \"MISCELLANEOUS\"})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otJe-8roCiGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## TEXT CLUSTERING\n",
        "\n",
        "# best LDA\n",
        "documents = df['text_clean'].sample(n=20000,replace=True).values.tolist()\n",
        "search_params = {'n_components': [10, 15, 20, 25, 30]}\n",
        "best_model_params, best_model_score, best_model_perplexity = best_LDA(search_params, documents)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOYbBQr0CtDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## TEXT CLUSTERING\n",
        "\n",
        "documents = df['text_clean'].astype(str)\n",
        "n_topics = 15\n",
        "n_words = 10\n",
        "\n",
        "# LDA\n",
        "LDA_topic, LDA_topic_words = LDA_model(documents, n_topics, n_words)\n",
        "df['LDA_topic'] = LDA_topic\n",
        "# kMeans\n",
        "#kMeans_topic = kMeans_topics(documents, n_topics)\n",
        "#df['kMeans_topic'] = kMeans_topic"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08WZpWyQCv36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "style.use(\"ggplot\")\n",
        "\n",
        "# STYLOMETRIC FEATURE EXTRACTION\n",
        "\n",
        "cmuDictionary = None\n",
        "texts = df['text'].tolist()\n",
        "start_time = time.time()\n",
        "vector = FeatureExtraction(texts, winSize=10, step=10)\n",
        "end_time = time.time()\n",
        "print(\"elapsed time: \"+str((end_time - start_time)/60)+\" min\")\n",
        "\n",
        "feature_names = pd.DataFrame(vector,\n",
        "                             columns =['Avg_wordLength', 'Avg_SentLenghtByCh', 'Avg_SentLenghtByWord', 'Avg_Syllable_per_Word', 'CountSpecialCharacter', \n",
        "                                       'CountPunctuation', 'CountFunctionalWords', 'typeTokenRatio', 'hapax', 'HonoreMeasureR', \n",
        "                                       'dihapax', 'SichelesMeasureS', 'YulesCharacteristicK','SimpsonsIndex', 'BrunetsMeasureW', \n",
        "                                       'ShannonEntropy', 'FleschReadingEase', 'FleschCincadeGradeLevel', 'dale_chall_readability_formula', 'GunningFoxIndex'])\n",
        "\n",
        "df = pd.concat([df.reset_index(drop=True), feature_names], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Win84GsQJezh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# STYLOMETRIC CLUSTERING\n",
        "\n",
        "vector=df.iloc[:, 13:]\n",
        "vector.fillna(0, inplace=True)\n",
        "vector = vector.values.tolist()\n",
        "#kMeans_ElbowMethod(np.array(vector), kmin=1,kmax=35)\n",
        "\n",
        "labels = kMeans_clusters(vector, K=15)\n",
        "df[\"stylometric_km_group\"] = labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7EM1pfSgBBf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CATEGORY CLASSIFICATION: data preparation\n",
        "\n",
        "## Prepare dataset\n",
        "selected = ['text_clean','text_stem','category_merged']\n",
        "# Create training dataframe\n",
        "df2=df[selected].sample(n=len(df)) #re-sort rows\n",
        "c_size=min(pd.value_counts(df2.category_merged)) #finds smallest category\n",
        "TestSize = 0.15 #sets maximun train size for the smallest category\n",
        "columns = df2.columns #gets columns names\n",
        "categories = df2['category_merged'].unique() # obtains nique values for categories\n",
        "df_train = pd.DataFrame(columns=columns) # creates empty pd.dataframe\n",
        "for cat in categories:\n",
        "  df_ = df2[df2.category_merged == cat][:int((1-TestSize)*c_size)] # selects rows for training per category\n",
        "  df_train = df_train.append(df_) # appends to training df\n",
        "\n",
        "#create test dataframe\n",
        "df_test = df.drop(df_train.index)\n",
        "df_test = df_test.sample(n = len(df_train))\n",
        "\n",
        "#fill NaN\n",
        "df_train = df_train.fillna(\"0000\")\n",
        "df_test = df_test.fillna(\"0000\")\n",
        "\n",
        "# Separate x from Y\n",
        "x_train = df_train['text_clean'].tolist()\n",
        "Y_train = df_train['category_merged'].tolist()\n",
        "x_test = df_test['text_clean'].tolist()\n",
        "Y_real = df_test['category_merged'].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7DdCrUXmLhh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRID OPTIMIZATION\n",
        "\n",
        "# Set pipeline:\n",
        "# 1. vectorize\n",
        "# 2. transform to TF-IDF\n",
        "# 3. Assign classifiers\n",
        "# 4. ensemble by voting\n",
        "\n",
        "classifiers = [('svm', SVC(C=1, kernel='linear')), ('rf', RandomForestClassifier(n_estimators=100)), ('nb', MultinomialNB()), ('lgb',LGBMClassifier(n_estimators=100))]\n",
        "param_grid = {'voting__weights': [[1, 1, 1, 0], [1, 1, 0, 1], [1, 0, 1, 1], [0, 1, 1, 1]]}\n",
        "clf = RandomForestClassifier(n_estimators=100)\n",
        "pl = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('voting', VotingClassifier(classifiers, voting=\"soft\"))])\n",
        "\n",
        "# Train grid model\n",
        "grid_search = GridSearchCV(pl, param_grid=param_grid, n_jobs=-1, verbose=10, scoring='accuracy')\n",
        "grid_search.fit(x_train, Y_train)\n",
        "cv_results = grid_search.cv_results_\n",
        "best_parameters = grid_search.best_estimator_.get_params()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4LTrbA1jIew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CATEGORY CLASSIFICATION\n",
        "\n",
        "# 1. vectorize\n",
        "# 2. transform to TF-IDF\n",
        "# 3. Assign classifiers\n",
        "# 4. ensemble by voting\n",
        "\n",
        "classifiers = [('svm', SVC(C=1, kernel='linear')), \n",
        "               ('rf', RandomForestClassifier(n_estimators=100)), \n",
        "               ('nb', MultinomialNB())]\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=100)\n",
        "pl = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), \n",
        "               ('voting', VotingClassifier(estimators=classifiers, voting='soft',weights=[1,1,1]))])\n",
        "# Train model\n",
        "model = pl.fit(x_train, Y_train)\n",
        "# saves model to file\n",
        "pickle.dump(model, open(\"/gdrive/My Drive/My Developments/Whale and Jaguar/model_ensemble.p\", \"wb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XMmn0XJmFAO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CATEGORY CLASSIFICATION\n",
        "\n",
        "Y = df['category_merged'].tolist()\n",
        "x = df['text_clean'].fillna(\"notexthere\").tolist()\n",
        "Prediction = model.predict(x)\n",
        "CM = sklearn.metrics.confusion_matrix(Y, Prediction)\n",
        "accuracy = sklearn.metrics.accuracy_score(Y, Prediction)\n",
        "print(\"accuracy: \"+str(accuracy))\n",
        "print(sklearn.metrics.classification_report(Y, Prediction))\n",
        "\n",
        "df[\"category_predicted\"] = Prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TG-3avrIW8c6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df.to_excel(r'/gdrive/My Drive/My Developments/Whale and Jaguar/News_Category_Dataset_processed.xlsx', index=False)\n",
        "LDA_topic_words.to_excel(r'/gdrive/My Drive/My Developments/Whale and Jaguar/News_Category_Dataset_LDA_topic_table.xlsx', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZGriZ1akWaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}